{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec6cb33-bab5-4c52-874f-580cf1a8a1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3a1ec4-04d1-44c2-a2a1-47f6fa8a6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0m\u001B[01;34martifacts\u001B[0m/        dvc.yaml  params.yaml       setup.py     \u001B[01;34mvenv\u001B[0m/\n",
      "\u001B[01;34mChicken_project\u001B[0m/  LICENSE   README.md         \u001B[01;34msrc\u001B[0m/\n",
      "\u001B[01;34mconfig\u001B[0m/           \u001B[01;34mlogs\u001B[0m/     requirements.txt  template.py\n",
      "\u001B[01;31mdata.zip\u001B[0m          main.py   \u001B[01;34mresearch\u001B[0m/         \u001B[01;34mtemplates\u001B[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9e5b68-6585-4977-bfa2-e1e7a0d98166",
   "metadata": {},
   "source": [
    "# Entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd104311-40da-479b-be02-9d14854ecd85",
   "metadata": {},
   "source": [
    "### callback entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48fd9595-d37f-4fd2-8bfa-81b1ff6cf853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class CallbacksConfig:\n",
    "    root_dir: Path\n",
    "    tensorboard_root_log_dir: Path\n",
    "    checkpoint_model_filepath: Path\n",
    "    \n",
    "    batch_size: int   # set batch size for training\n",
    "    epochs: int   # number of all epochs in training\n",
    "    patience: int   #number of epochs to wait to adjust lr if monitored value does not improve\n",
    "    stop_patience: int   # number of epochs to wait before stopping training if monitored value does not improve\n",
    "    threshold: float   # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
    "    factor: float   # factor to reduce lr by\n",
    "    ask_epoch: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15a24cbc-9846-465f-a867-be63f3800601",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path # to store the trained model\n",
    "    base_model_path: Path # load the base model\n",
    "    data_dir: Path # load data and csv\n",
    "    csv_dir: Path\n",
    "    img_size: list\n",
    "    channels: int\n",
    "    color: str\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    data_gen_path: Path # to save the data generators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa0044-1105-44b5-b4e4-d27bbe05fe96",
   "metadata": {},
   "source": [
    "# Config manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a3e64b6-53f9-4ebb-a2b2-a8177c857313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnClassifier.constants import *\n",
    "from cnnClassifier.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9bc3e484-889f-49b8-b3ab-1eba8a420495",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self, \n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_callback_config(self) -> CallbacksConfig:\n",
    "        config = self.config.callbacks\n",
    "        model_ckpt_dir = os.path.dirname(config.checkpoint_model_filepath)\n",
    "        create_directories([\n",
    "            Path(model_ckpt_dir),\n",
    "            Path(config.tensorboard_root_log_dir)\n",
    "        ])\n",
    "        param_config = self.params\n",
    "\n",
    "        callback_config = CallbacksConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            tensorboard_root_log_dir=Path(config.tensorboard_root_log_dir),\n",
    "            checkpoint_model_filepath=Path(config.checkpoint_model_filepath),\n",
    "            batch_size = param_config.batch_size,   # set batch size for training\n",
    "            epochs = param_config.epochs ,  # number of all epochs in training\n",
    "            patience = param_config.patience,   #number of epochs to wait to adjust lr if monitored value does not improve\n",
    "            stop_patience = param_config.stop_patience,   # number of epochs to wait before stopping training if monitored value does not improve\n",
    "            threshold = param_config.threshold ,  # if train accuracy is < threshold adjust monitor accuracy, else monitor validation loss\n",
    "            factor = param_config.factor,   # factor to reduce lr by\n",
    "            ask_epoch = param_config.ask_epoch,   # number of epochs to run before asking if you want to halt training\n",
    "        )\n",
    "\n",
    "        return callback_config\n",
    "\n",
    "        \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        training = self.config.training\n",
    "        params = self.params\n",
    "        \n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"Chicken-fecal-images\")\n",
    "        create_directories([\n",
    "            Path(training.root_dir)\n",
    "        ])\n",
    "\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            base_model_path=Path(training.base_model_path),\n",
    "            data_dir=Path(training.data_dir),\n",
    "            csv_dir=Path(training.csv_dir),\n",
    "            data_gen_path=Path(training.data_gen_path),\n",
    "            img_size=params.img_size,\n",
    "            channels=params.channels,\n",
    "            color=params.color,\n",
    "            epochs = params.epochs,\n",
    "            batch_size=params.batch_size\n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "014effdd-f01f-4f6b-9605-3240b2d843e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-10 12:08:29,492: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2023-12-10 12:08:29,495: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-12-10 12:08:29,495: INFO: common: created directory at: artifacts]\n",
      "[2023-12-10 12:08:29,496: INFO: common: created directory at: artifacts/training]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainingConfig(root_dir=PosixPath('artifacts/training'), trained_model_path=PosixPath('artifacts/training/trained_model.h5'), base_model_path=PosixPath('artifacts/base_model/base_model.h5'), data_dir=PosixPath('artifacts/data_ingestion/Train/Train'), csv_dir=PosixPath('artifacts/data_ingestion/Train/train_data.csv'), img_size=BoxList([224, 224]), channels=3, color='rgb', epochs=40, batch_size=40, data_gen_path=PosixPath('artifacts/training'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConfigurationManager().get_training_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f80fb9-8f1f-46cd-b6c8-1f614cd476e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72cfeea7-7c7b-4802-a89f-1b9c0a150243",
   "metadata": {},
   "source": [
    "# Component callback and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2eec4aae-f47c-410b-b3fe-f36a4e5226d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82d4f82a-832e-4b17-98d7-d384ced278b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, model, train_gen, config: CallbacksConfig):\n",
    "        super(MyCallback, self).__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.model = model\n",
    "        self.patience = config.patience # specifies how many epochs without improvement before learning rate is adjusted\n",
    "        self.stop_patience = config.stop_patience # specifies how many times to adjust lr without improvement to stop training\n",
    "        self.threshold = config.threshold # specifies training accuracy threshold when lr will be adjusted based on validation loss\n",
    "        self.factor = config.factor # factor by which to reduce the learning rate\n",
    "        self.epochs = config.epochs\n",
    "        self.ask_epoch = config.ask_epoch\n",
    "        self.ask_epoch_initial = config.ask_epoch # save this value to restore if restarting training\n",
    "        \n",
    "        self.batches = int(np.ceil(len(train_gen.labels) / config.batch_size))    # number of training batch to run per epoch\n",
    "        \n",
    "        # callback variables\n",
    "        self.count = 0 # how many times lr has been reduced without improvement\n",
    "        self.stop_count = 0\n",
    "        self.best_epoch = 1   # epoch with the lowest loss\n",
    "        self.initial_lr = float(tf.keras.backend.get_value(model.optimizer.lr)) # get the initial learning rate and save it\n",
    "        self.highest_tracc = 0.0 # set highest training accuracy to 0 initially\n",
    "        self.lowest_vloss = np.inf # set lowest validation loss to infinity initially\n",
    "        self.best_weights = self.model.get_weights() # set best weights to model's initial weights\n",
    "        self.initial_weights = self.model.get_weights()   # save initial weights if they have to get restored\n",
    "\n",
    "    # Define a function that will run when train begins\n",
    "    def on_train_begin(self, logs= None):\n",
    "        msg = 'Do you want model asks you to halt the training [y/n] ?'\n",
    "        print(msg)\n",
    "        ans = 'n' # can take input from user\n",
    "        if ans in ['Y', 'y']:\n",
    "            self.ask_permission = 1\n",
    "        elif ans in ['N', 'n']:\n",
    "            self.ask_permission = 0\n",
    "\n",
    "        msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor','% Improv', 'Duration')\n",
    "        print(msg)\n",
    "        self.start_time = time.time()\n",
    "\n",
    "\n",
    "    def on_train_end(self, logs= None):\n",
    "        stop_time = time.time()\n",
    "        tr_duration = stop_time - self.start_time\n",
    "        hours = tr_duration // 3600\n",
    "        minutes = (tr_duration - (hours * 3600)) // 60\n",
    "        seconds = tr_duration - ((hours * 3600) + (minutes * 60))\n",
    "\n",
    "        msg = f'training elapsed time was {str(hours)} hours, {minutes:4.1f} minutes, {seconds:4.2f} seconds)'\n",
    "        print(msg)\n",
    "\n",
    "        # set the weights of the model to the best weights\n",
    "        self.model.set_weights(self.best_weights)\n",
    "\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs= None):\n",
    "        # get batch accuracy and loss\n",
    "        acc = logs.get('accuracy') * 100\n",
    "        loss = logs.get('loss')\n",
    "\n",
    "        # prints over on the same line to show running batch count\n",
    "        msg = '{0:20s}processing batch {1:} of {2:5s}-   accuracy=  {3:5.3f}   -   loss: {4:8.5f}'.format(' ', str(batch), str(self.batches), acc, loss)\n",
    "        print(msg, '\\r', end= '')\n",
    "\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs= None):\n",
    "        self.ep_start = time.time()\n",
    "\n",
    "\n",
    "    # Define method runs on the end of each epoch\n",
    "    def on_epoch_end(self, epoch, logs= None):\n",
    "        ep_end = time.time()\n",
    "        duration = ep_end - self.ep_start\n",
    "\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.lr)) # get the current learning rate\n",
    "        current_lr = lr\n",
    "        acc = logs.get('accuracy')  # get training accuracy\n",
    "        v_acc = logs.get('val_accuracy')  # get validation accuracy\n",
    "        loss = logs.get('loss')  # get training loss for this epoch\n",
    "        v_loss = logs.get('val_loss')  # get the validation loss for this epoch\n",
    "\n",
    "        if acc < self.threshold: # if training accuracy is below threshold adjust lr based on training accuracy\n",
    "            monitor = 'accuracy'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "            else:\n",
    "                pimprov = (acc - self.highest_tracc ) * 100 / self.highest_tracc # define improvement of model progres\n",
    "\n",
    "            if acc > self.highest_tracc: # training accuracy improved in the epoch\n",
    "                self.highest_tracc = acc # set new highest training accuracy\n",
    "                self.best_weights = self.model.get_weights() # training accuracy improved so save the weights\n",
    "                self.count = 0 # set count to 0 since training accuracy improved\n",
    "                self.stop_count = 0 # set stop counter to 0\n",
    "                if v_loss < self.lowest_vloss:\n",
    "                    self.lowest_vloss = v_loss\n",
    "                self.best_epoch = epoch + 1  # set the value of best epoch for this epoch\n",
    "\n",
    "            else:\n",
    "                # training accuracy did not improve check if this has happened for patience number of epochs\n",
    "                # if so adjust learning rate\n",
    "                if self.count >= self.patience - 1: # lr should be adjusted\n",
    "                    lr = lr * self.factor # adjust the learning by factor\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "                    self.count = 0 # reset the count to 0\n",
    "                    self.stop_count = self.stop_count + 1 # count the number of consecutive lr adjustments\n",
    "                    self.count = 0 # reset counter\n",
    "                    if v_loss < self.lowest_vloss:\n",
    "                        self.lowest_vloss = v_loss\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment patience counter\n",
    "\n",
    "        else: # training accuracy is above threshold so adjust learning rate based on validation loss\n",
    "            monitor = 'val_loss'\n",
    "            if epoch == 0:\n",
    "                pimprov = 0.0\n",
    "\n",
    "            else:\n",
    "                pimprov = (self.lowest_vloss - v_loss ) * 100 / self.lowest_vloss\n",
    "\n",
    "            if v_loss < self.lowest_vloss: # check if the validation loss improved\n",
    "                self.lowest_vloss = v_loss # replace lowest validation loss with new validation loss\n",
    "                self.best_weights = self.model.get_weights() # validation loss improved so save the weights\n",
    "                self.count = 0 # reset count since validation loss improved\n",
    "                self.stop_count = 0\n",
    "                self.best_epoch = epoch + 1 # set the value of the best epoch to this epoch\n",
    "\n",
    "            else: # validation loss did not improve\n",
    "                if self.count >= self.patience - 1: # need to adjust lr\n",
    "                    lr = lr * self.factor # adjust the learning rate\n",
    "                    self.stop_count = self.stop_count + 1 # increment stop counter because lr was adjusted\n",
    "                    self.count = 0 # reset counter\n",
    "                    tf.keras.backend.set_value(self.model.optimizer.lr, lr) # set the learning rate in the optimizer\n",
    "\n",
    "                else:\n",
    "                    self.count = self.count + 1 # increment the patience counter\n",
    "\n",
    "                if acc > self.highest_tracc:\n",
    "                    self.highest_tracc = acc\n",
    "\n",
    "        msg = f'{str(epoch + 1):^3s}/{str(self.epochs):4s} {loss:^9.3f}{acc * 100:^9.3f}{v_loss:^9.5f}{v_acc * 100:^9.3f}{current_lr:^9.5f}{lr:^9.5f}{monitor:^11s}{pimprov:^10.2f}{duration:^8.2f}'\n",
    "        print(msg)\n",
    "\n",
    "        if self.stop_count > self.stop_patience - 1: # check if learning rate has been adjusted stop_count times with no improvement\n",
    "            msg = f' training has been halted at epoch {epoch + 1} after {self.stop_patience} adjustments of learning rate with no improvement'\n",
    "            print(msg)\n",
    "            self.model.stop_training = True # stop training\n",
    "\n",
    "        else:\n",
    "            if self.ask_epoch != None and self.ask_permission != 0:\n",
    "                if epoch + 1 >= self.ask_epoch:\n",
    "                    msg = 'enter H to halt training or an integer for number of epochs to run then ask again'\n",
    "                    print(msg)\n",
    "\n",
    "                    ans = input('')\n",
    "                    if ans == 'H' or ans == 'h':\n",
    "                        msg = f'training has been halted at epoch {epoch + 1} due to user input'\n",
    "                        print(msg)\n",
    "                        self.model.stop_training = True # stop training\n",
    "\n",
    "                    else:\n",
    "                        try:\n",
    "                            ans = int(ans)\n",
    "                            self.ask_epoch += ans\n",
    "                            msg = f' training will continue until epoch {str(self.ask_epoch)}'\n",
    "                            print(msg)\n",
    "                            msg = '{0:^8s}{1:^10s}{2:^9s}{3:^9s}{4:^9s}{5:^9s}{6:^9s}{7:^10s}{8:10s}{9:^8s}'.format('Epoch', 'Loss', 'Accuracy', 'V_loss', 'V_acc', 'LR', 'Next LR', 'Monitor', '% Improv', 'Duration')\n",
    "                            print(msg)\n",
    "\n",
    "                        except Exception:\n",
    "                            print('Invalid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0cb49a43-0998-4fe8-869a-618e7ab9a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareCallback:\n",
    "    def __init__(self, config: CallbacksConfig):\n",
    "        self.config = config\n",
    "\n",
    "    @property\n",
    "    def _create_tb_callbacks(self):\n",
    "        timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        tb_running_log_dir = os.path.join(\n",
    "            self.config.tensorboard_root_log_dir,\n",
    "            f\"tb_logs_at_{timestamp}\",\n",
    "        )\n",
    "        return tf.keras.callbacks.TensorBoard(log_dir=tb_running_log_dir)\n",
    "    \n",
    "\n",
    "    @property\n",
    "    def _create_ckpt_callbacks(self):\n",
    "        return tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=str(self.config.checkpoint_model_filepath),\n",
    "            save_best_only=True,\n",
    "            monitor='val_accuracy',\n",
    "            mode='max'\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_tb_ckpt_callbacks(self):\n",
    "        config = self.config\n",
    "        return [\n",
    "            self._create_tb_callbacks,\n",
    "            self._create_ckpt_callbacks\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c08e745c-d80d-4030-bff2-b6c8d8cbb6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     root_dir: Path\n",
    "#     trained_model_path: Path # to store the trained model\n",
    "#     base_model_path: Path # load the base model\n",
    "#     data_dir: Path # load data and csv\n",
    "#     csv_dir: Path\n",
    "# img_size\n",
    "# channels\n",
    "# color\n",
    "# batch_size\n",
    "# data_gen_path # to save the data generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b82939db-861c-402a-8993-4139d0224968",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Training:\n",
    "    def __init__(self, config: TrainingConfig):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.histroy = None\n",
    "        self.train_gen = None\n",
    "        self.valid_gen = None\n",
    "        self.test_gen = None\n",
    "    \n",
    "    def get_base_model(self):\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            self.config.base_model_path\n",
    "        )\n",
    "        return self.model\n",
    "\n",
    "    def split_csv_data(self):\n",
    "        df = pd.read_csv(self.config.csv_dir)\n",
    "        # change column name\n",
    "        df.columns = [\"filepaths\",\"labels\"]\n",
    "        df['filepaths'] = df['filepaths'].apply(lambda x : os.path.join(self.config.data_dir, x))\n",
    "    \n",
    "        # only extracting two types of images\n",
    "        filter = (df['labels'] == 'Coccidiosis') | (df['labels'] == 'Healthy')\n",
    "        df = df[filter]\n",
    "        \n",
    "        # Create train df\n",
    "        strat = df['labels']\n",
    "        train_df, dummy_df = train_test_split(df,  train_size= 0.8, shuffle= True, random_state= 123, stratify=strat)\n",
    "    \n",
    "        # valid and test dataframe\n",
    "        strat = dummy_df['labels']\n",
    "        valid_df, test_df = train_test_split(dummy_df,  train_size= 0.5, shuffle= True, random_state= 123, stratify= strat)\n",
    "        \n",
    "        self.save_train_valid_test_df(train_df, valid_df, test_df)\n",
    "        \n",
    "        return train_df, valid_df, test_df\n",
    "\n",
    "    \n",
    "    def save_train_valid_test_df(self, train_df, valid_df, test_df):\n",
    "        \n",
    "        # Save generators\n",
    "        path = os.path.join(self.config.data_gen_path,'train_df.pkl')\n",
    "        with open(path, 'wb') as f:\n",
    "            dill.dump(train_df, f)\n",
    "\n",
    "        path = os.path.join(self.config.data_gen_path,'valid_df.pkl')\n",
    "        with open(path, 'wb') as f:\n",
    "            dill.dump(valid_df, f)\n",
    "\n",
    "        path = os.path.join(self.config.data_gen_path,'test_df.pkl')\n",
    "        with open(path, 'wb') as f:\n",
    "            dill.dump(test_df, f)\n",
    "    \n",
    "    \n",
    "    def create_train_valid_test_generator(self, train_df, valid_df, test_df):\n",
    "        '''\n",
    "        This function takes train, validation, and test dataframe and fit them into image data generator, because model takes data from image data generator.\n",
    "        Image data generator converts images into tensors. \n",
    "        '''\n",
    "\n",
    "        # define model parameters\n",
    "        batch_size = self.config.batch_size\n",
    "        img_size = self.config.img_size\n",
    "        channels = self.config.channels\n",
    "        color = self.config.color\n",
    "        img_shape = (img_size[0], img_size[1], channels)\n",
    "    \n",
    "        ts_length = len(test_df)\n",
    "        test_batch_size = self.config.batch_size\n",
    "        test_steps = ts_length // test_batch_size\n",
    "        # This function which will be used in image data generator for data augmentation, it just take the image and return it again.\n",
    "        def scalar(img):\n",
    "            return img\n",
    "    \n",
    "        tr_gen = ImageDataGenerator(preprocessing_function= scalar, horizontal_flip= True, rescale = 1./255)\n",
    "        ts_gen = ImageDataGenerator(preprocessing_function= scalar, rescale = 1./255)\n",
    "    \n",
    "        train_gen = tr_gen.flow_from_dataframe(train_df, x_col = 'filepaths', y_col = 'labels', target_size= img_size, class_mode = \"binary\",\n",
    "                                              color_mode= color, shuffle= False, batch_size= batch_size)\n",
    "        valid_gen = ts_gen.flow_from_dataframe(valid_df, x_col = 'filepaths', y_col = 'labels', target_size= img_size, class_mode = \"binary\",\n",
    "                                              color_mode= color, shuffle= False, batch_size= batch_size)\n",
    "         # Note: we will use custom test_batch_size, and make shuffle= false\n",
    "        test_gen = ts_gen.flow_from_dataframe(test_df, x_col = 'filepaths', y_col = 'labels', target_size= img_size, class_mode = \"binary\",\n",
    "                                             color_mode= color, shuffle= False, batch_size= test_batch_size)\n",
    "        self.train_gen = train_gen\n",
    "        self.valid_gen = valid_gen\n",
    "        self.test_gen = test_gen\n",
    "        \n",
    "        return train_gen, valid_gen, test_gen\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        model.save(path)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def train(self, callback_list: list):\n",
    "\n",
    "        self.history = self.model.fit(x= self.train_gen, epochs= self.config.epochs, verbose= 0, callbacks= callback_list,\n",
    "                        validation_data= self.valid_gen, validation_steps= None, shuffle= False)\n",
    "        \n",
    "        self.save_model(\n",
    "            path=self.config.trained_model_path,\n",
    "            model=self.model\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "722ecf71-100a-48da-930b-9d69810c0002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-12-10 12:20:38,504: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2023-12-10 12:20:38,508: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2023-12-10 12:20:38,509: INFO: common: created directory at: artifacts]\n",
      "[2023-12-10 12:20:38,510: INFO: common: created directory at: artifacts/training]\n",
      "Found 3904 validated image filenames belonging to 2 classes.\n",
      "Found 488 validated image filenames belonging to 2 classes.\n",
      "Found 488 validated image filenames belonging to 2 classes.\n",
      "[2023-12-10 12:20:38,775: INFO: common: created directory at: artifacts/callbacks/checkpoint_dir]\n",
      "[2023-12-10 12:20:38,776: INFO: common: created directory at: artifacts/callbacks/tensorboard_log_dir]\n",
      "Do you want model asks you to halt the training [y/n] ?\n",
      " Epoch     Loss   Accuracy  V_loss    V_acc     LR     Next LR  Monitor  % Improv  Duration\n",
      " 1 /2      2.011   88.858   1.93559  65.574   0.00100  0.00100  accuracy     0.00    121.04 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucky/Documents/data_science_interview/Udemy/My_Projects/chicken_binary/venv/lib/python3.11/site-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2 /2      0.382   93.391   0.25612  93.033   0.00100  0.00100  val_loss    86.77    125.20 \n",
      "training elapsed time was 0.0 hours,  4.0 minutes, 7.40 seconds)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = Training(config=training_config)\n",
    "    model = training.get_base_model()\n",
    "    train_df, valid_df, test_df = training.split_csv_data()\n",
    "    train_gen, valid_gen, test_gen = training.create_train_valid_test_generator(train_df, valid_df, test_df)\n",
    "\n",
    "    callbacks_config = config.get_callback_config()\n",
    "    prepare_callbacks = PrepareCallback(config=callbacks_config)\n",
    "    callback_list = prepare_callbacks.get_tb_ckpt_callbacks()\n",
    "    \n",
    "    callback_list = [MyCallback(model, train_gen, callbacks_config)] + callback_list\n",
    "\n",
    "\n",
    "    \n",
    "    training.train(\n",
    "        callback_list=callback_list\n",
    "    )\n",
    "    \n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca10eecd-5d7b-446c-b319-cd2fc2a916c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c86e3efb-7a79-418c-a762-f3bf9a503b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e8966-bfa3-4960-bbcb-9e25717d9fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10f6737-1e20-47bc-ba7a-e0a57e396f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
